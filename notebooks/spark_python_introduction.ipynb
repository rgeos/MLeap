{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction to ![spark](../data/pics/spark.png) using ![scala](../data/pics/python.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the version of spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRange = spark.range(1000).toDF(\"range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|range|\n",
      "+-----+\n",
      "|    0|\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "|    4|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myRange.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some transformations\n",
    "\n",
    "- Spark will not act on transformations.\n",
    "- All transformations in Spark are lazy => we wait until an action is called\n",
    "- Spark will create a DAG (Directed Acyclic Graph) and act upon the source data\n",
    "- Spark will optimize the pipeline\n",
    "- Examples of [transformations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations): map, filter, join, groupBy, sortByKey ... etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "div3 = myRange.where(\"range % 3 = 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action\n",
    "\n",
    "- Trigger the computation on the logic transformation\n",
    "- Examples of [actions](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions): reduce, count, collect, take, saveAsTextFile ... etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "334"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div3.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the results using the Spark UI: http://localhost:4040/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|range|\n",
      "+-----+\n",
      "|    0|\n",
      "|    3|\n",
      "|    6|\n",
      "|    9|\n",
      "|   12|\n",
      "|   15|\n",
      "|   18|\n",
      "|   21|\n",
      "|   24|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "div3.filter(div3[\"range\"] < 25).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div3.filter(\"range > 10 AND range < 25\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = spark.read\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .csv(\"../data/titanic.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are calling this data frequently, it is better to cache it for faster access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(PassengerId=1, Survived=0, Pclass=3, Name='Braund, Mr. Owen Harris', Sex='male', Age=22.0, SibSp=1, Parch=0, Ticket='A/5 21171', Fare=7.25, Cabin=None, Embarked='S'),\n",
       " Row(PassengerId=2, Survived=1, Pclass=1, Name='Cumings, Mrs. John Bradley (Florence Briggs Thayer)', Sex='female', Age=38.0, SibSp=1, Parch=0, Ticket='PC 17599', Fare=71.2833, Cabin='C85', Embarked='C'),\n",
       " Row(PassengerId=3, Survived=1, Pclass=3, Name='Heikkinen, Miss. Laina', Sex='female', Age=26.0, SibSp=0, Parch=0, Ticket='STON/O2. 3101282', Fare=7.925, Cabin=None, Embarked='S')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data.take(3)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+--------+------+\n",
      "| Age|   Sex|Survived|Pclass|\n",
      "+----+------+--------+------+\n",
      "|22.0|  male|       0|     3|\n",
      "|38.0|female|       1|     1|\n",
      "|26.0|female|       1|     3|\n",
      "+----+------+--------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data.show(3, truncate=False)\n",
    "#data.select(\"Survived\", \"Sex\", \"Age\").show(10)\n",
    "#data.show(3)\n",
    "data.select(data.Age, data.Sex, data[\"Survived\"], \"Pclass\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId',\n",
       " 'Survived',\n",
       " 'Pclass',\n",
       " 'Name',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Ticket',\n",
       " 'Fare',\n",
       " 'Cabin',\n",
       " 'Embarked']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------------+\n",
      "|summary|   Sex|               Age|\n",
      "+-------+------+------------------+\n",
      "|  count|   891|               714|\n",
      "|   mean|  null| 29.69911764705882|\n",
      "| stddev|  null|14.526497332334035|\n",
      "|    min|female|              0.42|\n",
      "|    max|  male|              80.0|\n",
      "+-------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# summary statistics about the data\n",
    "data.describe('Sex', \"Age\").show()\n",
    "#data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames overview\n",
    "\n",
    " - __Immutable__: once created cannot be changed. we applying transformation to the existing DF, a new one will be created\n",
    " - __Lazy__: unless there is an action performed on the DF, no transformation will be computed\n",
    " - __Distributed__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating DataFrames (or SparkSQL)\n",
    "\n",
    "- __sort()__ :\n",
    "\n",
    "    - When we are using `sort`, spark will not perform anything on the data, because it is just a transformation. However, it will create a plan for when an action is called. We can use `explain` to see the plan.\n",
    "    - When reading the `explain`, on top we have the end result and at the bottom is the data we start with.\n",
    "    - Only when we call an action on the data frame, the entire DAG is computed as shown in the `explain` pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Sort [Survived#92 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(Survived#92 ASC NULLS FIRST, 200)\n",
      "   +- InMemoryTableScan [PassengerId#91, Survived#92, Pclass#93, Name#94, Sex#95, Age#96, SibSp#97, Parch#98, Ticket#99, Fare#100, Cabin#101, Embarked#102]\n",
      "         +- InMemoryRelation [PassengerId#91, Survived#92, Pclass#93, Name#94, Sex#95, Age#96, SibSp#97, Parch#98, Ticket#99, Fare#100, Cabin#101, Embarked#102], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "               +- *FileScan csv [PassengerId#91,Survived#92,Pclass#93,Name#94,Sex#95,Age#96,SibSp#97,Parch#98,Ticket#99,Fare#100,Cabin#101,Embarked#102] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/resueman/Dev/spark/MLeap/data/titanic.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<PassengerId:int,Survived:int,Pclass:int,Name:string,Sex:string,Age:double,SibSp:int,Parch:...\n"
     ]
    }
   ],
   "source": [
    "data.sort(\"Survived\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|   Sex|Survived|\n",
      "+------+--------+\n",
      "|female|       1|\n",
      "|female|       1|\n",
      "|female|       1|\n",
      "|female|       1|\n",
      "+------+--------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "data.sort(desc(\"Survived\")).select(\"Sex\", \"Survived\").show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating DataFrames (or SparkSQL)\n",
    "\n",
    "- __createOrReplaceTempView()__\n",
    "    - Spark SQL will create a temporary table from your DataFrame, which you can query with normal SQL\n",
    "    - The temporary table can be manipulated with DataFrame code also\n",
    "    - There is no performance difference between SQL and DF code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.createOrReplaceTempView(\"titanic_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is a SparSQL query\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Sex, Survived, count(Survived) as count FROM titanic_data GROUP BY Sex, Survived ORDER BY Sex\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is a Spark DataFrame query\n",
    "data.groupBy(\"Sex\", \"Survived\").count().sort(\"Sex\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating DataFrames (or SparkSQL)\n",
    "\n",
    "- __crosstab__(*col1, col2*)\n",
    "    - pairwise frequency (contigency table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.crosstab(\"Sex\", \"Survived\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Manipulating DataFrames (or SparkSQL)\n",
    "\n",
    "- __distinct()__\n",
    "    - this will return a new DF containing the distinct rows in the original DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.select('Embarked').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating DataFrames (or SparkSQL)\n",
    "\n",
    "- __dropna__(*how='any', thresh=None, subset=None*)\n",
    "    - this will return a new DF omitting the rows containing null values\n",
    "    \n",
    "\n",
    "- __fillna__(*value, subset=None*)\n",
    "    - it will replace null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.count(), data.dropna(subset=\"Embarked\").count()\n",
    "#data.count(), data.dropna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.fillna(\"X\", subset=\"Embarked\").select(\"Embarked\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating DataFrames (or SparkSQL)\n",
    "\n",
    "- __filter__(*condition*)\n",
    "    - this will filter rows given a certain condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data.filter(data.Sex == \"male\").count()\n",
    "# data.filter(data[\"Sex\"] == \"female\").count()\n",
    "data.filter(data.Age < 25).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating DataFrames (or SparkSQL)\n",
    "\n",
    "- __groupBy__(*\\*cols*)\n",
    "    - groups the specified columns and runs aggregations on it\n",
    "    \n",
    "    \n",
    "- __agg__(*\\*expression*)\n",
    "    - aggregating on a DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.groupBy(\"Sex\").agg({\"Age\": \"average\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.agg({\"Age\": \"max\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.groupBy(\"Sex\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating DataFrames (or SparkSQL)\n",
    "\n",
    "Something more complex:\n",
    " - transform the data frame into RDD (resilient distributed dataset)\n",
    " - apply a mapping function to each row in the data frame\n",
    " - transform it back to a data frame\n",
    " - rename the column to `gender`\n",
    " - join the newly formed data frame with the original data\n",
    " - drop the `id` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getGender(string):\n",
    "    if(string == 'male'): return 0\n",
    "    elif(string == 'female'): return 1\n",
    "    else: return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = data.select(\"PassengerId\",\"Sex\").rdd.map(lambda x: (x.PassengerId,getGender(x.Sex)))\n",
    "df = spark.createDataFrame(rdd, [\"id\", \"gender\"])\n",
    "data.join(df, data.PassengerId == df.id, how='inner').drop(\"id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating DataFrames (or SparkSQL)\n",
    "\n",
    "Same thing as above, but using UDF (user defined functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "udf_gender = udf(lambda x: getGender(x))\n",
    "data.withColumn('Gender', udf_gender(data.Sex)).show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
